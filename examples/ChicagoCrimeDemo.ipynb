{"nbformat_minor": 0, "cells": [{"source": "### Let's first configure the Spark cluster\nAdd the jars for H2O sparkling water and the spark-csv. Also change the driver Memory to the size of the VM of the clusters.\n<br><b>Make sure that you are adding the right Sparkling Water version and setting the driver and executor memory to about 80% of the RAM of the VM sizes selected on cluster creation </b>\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\":{\n        \"spark.jars.packages\":\"ai.h2o:sparkling-water-core_2.10:1.6.8,com.databricks:spark-csv_2.10:1.5.0\"\n    },\n    \"driverMemory\":\"10G\",\n    \"executorMemory\":\"10G\"\n}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Here we add the pySpark egg file from the downloaded H2O Sparkling water distribution.\n<br><b> Make sure that file name of the egg file below matches the downloaded distribution of Sparkling Water</b>", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "sc.addPyFile('wasb:///HdiNotebooks/H2O-Sparkling-Water/py/dist/h2o_pysparkling_1.6-1.6.8-py2.7.egg')\nimport os\nos.environ[\"PYTHON_EGG_CACHE\"] = \"~/\"", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Now the coding starts..", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#Initiate H2OContext on top of Spark\nfrom pysparkling import *\nhc = H2OContext.getOrCreate(sc)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# This is just helper function returning relative path to data files within sparkling-water project directories\ndef _locate(example_name): \n    return \"wasb:///HdiNotebooks/H2O-Sparkling-Water/examples/smalldata/\" + example_name \n\n\n# Define file names\nchicagoAllWeather = \"chicagoAllWeather.csv\"\nchicagoCensus = \"chicagoCensus.csv\"\nchicagoCrimes10k = \"chicagoCrimes10k.csv\"\n\n# Add files to Spark Cluster\nsc.addFile(_locate(chicagoAllWeather))\nsc.addFile(_locate(chicagoCensus))\nsc.addFile(_locate(chicagoCrimes10k))\n\n# And import them into H2O\nfrom pyspark import SparkFiles\nimport h2o\n# Since we have already loaded files into spark, we have to use h2o.upload_file instead of \n# h2o.import_file since h2o.import_file expects cluster-relative path (ie. the file on this\n# path can be accessed from all the machines on the cluster) but SparkFiles.get(..) already\n# give us relative path to the file on a current node which h2o.upload_file can handle ( it\n# uploads file located on current node and distributes it to the H2O cluster)\nf_weather = h2o.upload_file(SparkFiles.get(chicagoAllWeather))\nf_census = h2o.upload_file(SparkFiles.get(chicagoCensus))\nf_crimes = h2o.upload_file(SparkFiles.get(chicagoCrimes10k), col_types = {\"Date\": \"string\"})", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "f_weather.show()\nf_census.show()\nf_crimes.show()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Set time zone to UTC for date manipulation\nh2o.set_timezone(\"Etc/UTC\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Transform weather table\n## Remove 1st column (date)\nf_weather = f_weather[1:]", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Transform census table\n## Remove all spaces from column names (causing problems in Spark SQL)\ncol_names = map(lambda s: s.strip().replace(' ', '_').replace('+','_'), f_census.col_names)\n\n## Update column names in the table\n#f_weather.names = col_names\nf_census.names = col_names", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Transform crimes table\n\n## Drop useless columns\nf_crimes = f_crimes[2:]\n\n## Replace ' ' by '_' in column names\ncol_names = map(lambda s: s.replace(' ', '_'), f_crimes.col_names)\nf_crimes.names = col_names\n\n## Refine date column\ndef refine_date_col(data, col, pattern):\n    data[col]         = data[col].as_date(pattern)\n    data[\"Day\"]       = data[col].day()\n    data[\"Month\"]     = data[col].month()\n    data[\"Year\"]      = data[col].year()\n    data[\"WeekNum\"]   = data[col].week()\n    data[\"WeekDay\"]   = data[col].dayOfWeek()\n    data[\"HourOfDay\"] = data[col].hour()\n    \n    data.describe() # HACK: Force evaluation before ifelse and cut. See PUBDEV-1425.\n        \n    # Create weekend and season cols\n    data[\"Weekend\"] = ((data[\"WeekDay\"] == \"Sun\") | (data[\"WeekDay\"] == \"Sat\"))\n    data[\"Season\"] = data[\"Month\"].cut([0, 2, 5, 7, 10, 12], [\"Winter\", \"Spring\", \"Summer\", \"Autumn\", \"Winter\"])\n    \nrefine_date_col(f_crimes, \"Date\", \"%m/%d/%Y %I:%M:%S %p\")\nf_crimes = f_crimes.drop(\"Date\")\nf_crimes.describe()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Expose H2O frames as Spark DataFrame\n\ndf_weather = hc.as_spark_frame(f_weather)\ndf_census = hc.as_spark_frame(f_census)\ndf_crimes = hc.as_spark_frame(f_crimes)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "df_weather.show()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Use Spark SQL to join datasets\n\n## Register DataFrames as tables in SQL context\nsqlContext.registerDataFrameAsTable(df_weather, \"chicagoWeather\")\nsqlContext.registerDataFrameAsTable(df_census, \"chicagoCensus\")\nsqlContext.registerDataFrameAsTable(df_crimes, \"chicagoCrime\")\n\n\ncrimeWithWeather = sqlContext.sql(\"\"\"SELECT\na.Year, a.Month, a.Day, a.WeekNum, a.HourOfDay, a.Weekend, a.Season, a.WeekDay,\na.IUCR, a.Primary_Type, a.Location_Description, a.Community_Area, a.District,\na.Arrest, a.Domestic, a.Beat, a.Ward, a.FBI_Code,\nb.minTemp, b.maxTemp, b.meanTemp,\nc.PERCENT_AGED_UNDER_18_OR_OVER_64, c.PER_CAPITA_INCOME, c.HARDSHIP_INDEX,\nc.PERCENT_OF_HOUSING_CROWDED, c.PERCENT_HOUSEHOLDS_BELOW_POVERTY,\nc.PERCENT_AGED_16__UNEMPLOYED, c.PERCENT_AGED_25__WITHOUT_HIGH_SCHOOL_DIPLOMA\nFROM chicagoCrime a\nJOIN chicagoWeather b\nON a.Year = b.year AND a.Month = b.month AND a.Day = b.day\nJOIN chicagoCensus c\nON a.Community_Area = c.Community_Area_Number\"\"\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "crimeWithWeather.show()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Publish Spark DataFrame as H2OFrame with given name\ncrimeWithWeatherHF = hc.as_h2o_frame(crimeWithWeather, \"crimeWithWeatherTable\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Transform selected String columns to categoricals\ncrimeWithWeatherHF[\"Arrest\"] = crimeWithWeatherHF[\"Arrest\"].asfactor()\ncrimeWithWeatherHF[\"Season\"] = crimeWithWeatherHF[\"Season\"].asfactor()\ncrimeWithWeatherHF[\"WeekDay\"] = crimeWithWeatherHF[\"WeekDay\"].asfactor()\ncrimeWithWeatherHF[\"Primary_Type\"] = crimeWithWeatherHF[\"Primary_Type\"].asfactor()\ncrimeWithWeatherHF[\"Location_Description\"] = crimeWithWeatherHF[\"Location_Description\"].asfactor()\ncrimeWithWeatherHF[\"Domestic\"] = crimeWithWeatherHF[\"Domestic\"].asfactor()", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Split frame into two - we use one as the training frame and the second one as the validation frame\nsplits = crimeWithWeatherHF.split_frame(ratios=[0.8])\ntrain = splits[0]\ntest = splits[1]\n\n# Prepare column names\npredictor_columns = train.drop(\"Arrest\").col_names\nresponse_column = \"Arrest\"", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Create and train GBM model\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\n\n# Prepare model based on the given set of parameters\ngbm_model = H2OGradientBoostingEstimator(  ntrees       = 50,\n                                     max_depth    = 3,\n                                     learn_rate   = 0.1,\n                                     distribution = \"bernoulli\"\n                                 )\n\n# Train the model\ngbm_model.train(x            = predictor_columns,\n            y                = response_column,\n            training_frame   = train,\n            validation_frame = test\n         )", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Show GBM model performance\ngbm_model.model_performance(test)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Create and train deeplearning model\nfrom h2o.estimators.deeplearning import H2ODeepLearningEstimator\n\n# Prepare model based on the given set of parameters\ndl_model = H2ODeepLearningEstimator()\n\n# Train the model\ndl_model.train(x            = predictor_columns,\n            y                = response_column,\n            training_frame   = train,\n            validation_frame = test\n            )", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Show deeplearning model performance\ndl_model.model_performance(test)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Create crime class which is used as a data holder on which prediction is done\nfrom datetime import datetime\nfrom pytz import timezone\nfrom pyspark.sql import Row\n\ndef get_season(dt):\n    if (dt >= 3 and dt <= 5):\n        return \"Spring\"\n    elif (dt >= 6 and dt <= 8):\n        return \"Summer\"\n    elif (dt >= 9 and dt <= 10):\n        return \"Autumn\"\n    else:       \n        return \"Winter\"\n    \ndef crime(date,\n        iucr,\n        primaryType,\n        locationDescr,\n        domestic,\n        beat,\n        district,\n        ward,\n        communityArea,\n        fbiCode,\n        minTemp = 77777,\n        maxTemp = 77777,\n        meanTemp = 77777,\n        datePattern = \"%d/%m/%Y %I:%M:%S %p\",\n        dateTimeZone = \"Etc/UTC\"):\n\n    dt = datetime.strptime(\"02/08/2015 11:43:58 PM\",'%d/%m/%Y %I:%M:%S %p')\n    dt.replace(tzinfo=timezone(\"Etc/UTC\"))\n\n    crime = Row(\n        Year = dt.year,\n        Month = dt.month,\n        Day = dt.day,\n        WeekNum = dt.isocalendar()[1],\n        HourOfDay = dt.hour,\n        Weekend = 1 if dt.weekday() == 5 or dt.weekday() == 6 else 0,\n        Season = get_season(dt.month),\n        WeekDay = dt.strftime('%a'),  #gets the day of week in short format - Mon, Tue ...\n        IUCR = iucr,\n        Primary_Type = primaryType,\n        Location_Description = locationDescr,\n        Domestic = True if domestic else False,\n        Beat = beat,\n        District = district,\n        Ward = ward,\n        Community_Area = communityArea,\n        FBI_Code = fbiCode,\n        minTemp = minTemp,\n        maxTemp = maxTemp,\n        meanTemp = meanTemp\n    )\n    return crime", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Create crime examples\ncrime_examples = [\n  crime(\"02/08/2015 11:43:58 PM\", 1811, \"NARCOTICS\", \"STREET\",False, 422, 4, 7, 46, 18),\n  crime(\"02/08/2015 11:00:39 PM\", 1150, \"DECEPTIVE PRACTICE\", \"RESIDENCE\",False, 923, 9, 14, 63, 11)]", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# For given crime and model return probability of crime.\ndef score_event(crime, model, censusTable):\n    rdd = sc.parallelize([crime])\n    crime_frame = sqlContext.createDataFrame(rdd)\n    # Join table with census data\n    df_row = censusTable.join(crime_frame).where(\"Community_Area = Community_Area_Number\")  \n    row = hc.as_h2o_frame(df_row)\n    row[\"Season\"] = row[\"Season\"].asfactor()\n    row[\"WeekDay\"] = row[\"WeekDay\"].asfactor()\n    row[\"Primary_Type\"] = row[\"Primary_Type\"].asfactor()\n    row[\"Location_Description\"] = row[\"Location_Description\"].asfactor()\n    row[\"Domestic\"] = row[\"Domestic\"].asfactor()\n\n    predictTable = model.predict(row)\n    probOfArrest = predictTable[\"true\"][0,0]\n    return probOfArrest\n\nfor crime in crime_examples:\n    arrestProbGBM = 100*score_event(crime, gbm_model, df_census)\n    arrestProbDLM = 100*score_event(crime, dl_model, df_census)\n\n    print(\"\"\"\n       |Crime: \"\"\"+str(crime)+\"\"\"\n       |  Probability of arrest best on DeepLearning: \"\"\"+str(arrestProbDLM)+\"\"\"\n       |  Probability of arrest best on GBM: \"\"\"+str(arrestProbGBM)+\"\"\"\n        \"\"\")", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"name": "python"}}, "anaconda-cloud": {}}}